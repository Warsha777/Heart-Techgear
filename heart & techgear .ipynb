{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6e88af",
   "metadata": {},
   "source": [
    "### Statistics Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3484730",
   "metadata": {},
   "source": [
    "#### Consider ‚Äúheart.xls‚Äù dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "652c0bc7-04a1-4329-939f-f3e0d59644d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel(\"C:\\\\Users\\\\warshape\\\\Desktop\\\\heart.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9acdbae",
   "metadata": {},
   "source": [
    "#### a) Consider the entire dataset. Is there any difference between the presence of heart disease (no/less chance / more chance) of female patients and male patients? (use a statistical test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bb6a2fd-ecd6-48b6-b41e-8db36fc0e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a contingency table for sex vs. target\n",
    "contingency_table = pd.crosstab(df['sex'], df['target'])\n",
    "# Perform Chi-Square Test\n",
    "chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"Chi-Square Statistic: {chi2}\")\n",
    "print(f\"P-value: {p}\")\n",
    "print(\"Significant relationship between gender and heart disease.\" if p < 0.05 else \"No significant relationship.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f086186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Contingency Table, Row Percentages, and Standardized Residuals\n",
    "print(f\"üî∂ Contingency Table \\n{contingency_table}\\n\")\n",
    "print(f\"üî∂ Row Percentages (%) \\n{(contingency_table.div(contingency_table.sum(axis=1), axis=0) * 100).round(2)}\\n\")\n",
    "print(f\"üî∂ Standardized Residuals \\n{((contingency_table - expected) / (expected**0.5)).round(2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6742709",
   "metadata": {},
   "source": [
    "#### b) Do you think normalization or standardization techniques would help to draw meaningful insights from the dataset? If so, what are those?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16091236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "stat, p_value = shapiro(df)\n",
    "print(f\"Shapiro-Wilk Test: W={stat}, p-value={p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c9802ec-d7ff-4164-8d87-78ed3b9049f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "# Check whether its continuous\n",
    "continuous_columns = []\n",
    "\n",
    "for col in numerical_columns:\n",
    "    # If the column has more than a set threshold of unique values, it is likely continuous\n",
    "    if df[col].nunique() > 20:  # you can adjust the threshold\n",
    "        continuous_columns.append(col)\n",
    "        \n",
    "# Display the continuous columns\n",
    "print(\"Continuous Numerical Columns:\")\n",
    "print(continuous_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12837a52-7ebb-4aa2-9f22-79a06f102ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize selected columns\n",
    "cols_to_standardize = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "# Apply StandardScaler ## Z-score standardization\n",
    "scaler = StandardScaler()\n",
    "df_standardized = df.copy()\n",
    "df_standardized[cols_to_standardize] = scaler.fit_transform(df[cols_to_standardize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7b3faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standardized ##### the standardized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bbfe129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify standardization\n",
    "df_standardized[cols_to_standardize].describe().loc[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e001f",
   "metadata": {},
   "source": [
    "Insights that we can gather from plotting after standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0d37b48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Feature Comparison: Scatter Plot Matrix\n",
    "sns.pairplot(df_standardized[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']])\n",
    "plt.suptitle('Feature Comparison After Standardization', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7041dc50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correlation: Heatmap\n",
    "correlation_matrix = df_standardized[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix After Standardization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6cdf601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Boxplot \n",
    "sns.boxplot(data=df_standardized[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']])\n",
    "plt.title('Boxplot of Features After Standardization')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "443da2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Clustering: K-Means Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df_standardized['Cluster'] = kmeans.fit_predict(df_standardized[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e60e1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for clustering visualization (2D projection for simplicity)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=df_standardized['age'], y=df_standardized['thalach'], hue=df_standardized['Cluster'], palette='Set1', marker='o')\n",
    "plt.title('K-Means Clustering of Patients (Age vs Thalach)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd5c62",
   "metadata": {},
   "source": [
    "#### c) Perform suitable regression analysis and develop a predictive model. Clearly mention the model, feature selection method and variables in the model. Further, interpret the model parameters."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c684159b-26b1-4aa0-9272-1a9c121ad1ce",
   "metadata": {},
   "source": [
    "A Logistic Regression Model can be developed since the target variable is binary and it's a regression analysis but a classification model\n",
    "Recursive Feature Elimination (RFE) can be used to select the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da71eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define independent variables (features)\n",
    "X = df_standardized.drop(columns=['target'])  # Drop target column to keep only features\n",
    "print('independent variables:', X.columns.tolist())\n",
    "\n",
    "# Define dependent variable (target)\n",
    "y = df_standardized['target']  \n",
    "print('\\ndependent variable:', y.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b2f2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Initialize logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Apply RFE for feature selection\n",
    "rfe = RFE(logreg, n_features_to_select=5)  # Select the top 5 most relevant features\n",
    "X_selected = rfe.fit_transform(X, y)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print(\"Selected Features:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54dd35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape before feature selection:\", X.shape)\n",
    "print(\"Shape after feature selection:\", X_selected.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a134011f",
   "metadata": {},
   "source": [
    "Split Data into Training and Testing Sets\n",
    "We split the data into 80% training and 20% testing to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ed8c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "542c9026",
   "metadata": {},
   "source": [
    "Train the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c99ecd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44a2693e",
   "metadata": {},
   "source": [
    "Interpret Model Parameters\n",
    "The coefficients (model.coef_) indicate how much each feature influences the probability of y = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3d933b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model coefficients\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Create a DataFrame to display coefficients\n",
    "coef_df = pd.DataFrame({'Feature': selected_features, 'Coefficient': coefficients})\n",
    "\n",
    "# Display coefficients\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a73aa61",
   "metadata": {},
   "source": [
    "#### d) Assess the model's predictive power using appropriate evaluation tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3bf9d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. Accuracy Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "#### 2. Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "#### 3. Precision, Recall, and F1-Score\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "#### 4. ROC-AUC Score\n",
    "auc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print(f\"AUC Score: {auc_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ff63f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b6b1207",
   "metadata": {},
   "source": [
    "### Statistics Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4d2f8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro, f_oneway\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"C:\\\\Users\\\\warshape\\\\Desktop\\\\techgear_sales.xlsx\"  # Update with your actual file path\n",
    "df = pd.read_excel(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e3b37",
   "metadata": {},
   "source": [
    "#### a) Create a histogram of purchase amounts and comment on the shape of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb86fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Histogram of Purchase Amounts ###\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df['purchase_amount'], bins=30, kde=True, color='skyblue')\n",
    "plt.xlabel(\"Purchase Amount (USD)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Purchase Amounts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b6be7a",
   "metadata": {},
   "source": [
    "#### b) What is the probability that a randomly selected purchase was made during a promotional period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbdae6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Probability of Purchase During a Promotional Period ###\n",
    "promo_probability = df['promo_period'].mean()\n",
    "print(f\"Probability of purchase during a promotional period: {promo_probability:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b93c3",
   "metadata": {},
   "source": [
    "#### c) Calculate the conditional probability that a purchase amount exceeds 1000 dollars given that it was made during a promotional period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "235f2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conditional Probability P(purchase_amount > 1000 | promo_period = 1) ###\n",
    "promo_purchases = df[df['promo_period'] == 1]\n",
    "prob_exceeds_1000_given_promo = (promo_purchases['purchase_amount'] > 1000).mean()\n",
    "print(f\"Conditional Probability P(Purchase > 1000 | Promo): {prob_exceeds_1000_given_promo:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277024de",
   "metadata": {},
   "source": [
    "#### d) Test whether purchase amounts follow a normal distribution using an appropriate statistical test. State your null and alternative hypotheses and interpret the results at a 5% significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110a49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test for Normality (Shapiro-Wilk Test) ###\n",
    "stat, p_value = shapiro(df['purchase_amount'])\n",
    "alpha = 0.05  # 5% significance level\n",
    "\n",
    "print(\"Normality Test Results:\")\n",
    "print(f\"Test Statistic: {stat:.4f}, P-Value: {p_value:.4f}\")\n",
    "\n",
    "if p_value > alpha:\n",
    "    print(\"Fail to reject H0: Data appears to be normally distributed.\")\n",
    "else:\n",
    "    print(\"Reject H0: Data does not follow a normal distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d26d50",
   "metadata": {},
   "source": [
    "#### e) Create age groups for customers (18-25, 26-35, 36-45, 46+) and calculate the mean purchase amount for each group. Conduct a one-way ANOVA test to determine if there are significant differences in purchase amounts across age groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22551758",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ANOVA Test for Purchase Amounts Across Age Groups ###\n",
    "# Define age groups\n",
    "bins = [18, 25, 35, 45, float('inf')]\n",
    "labels = [\"18-25\", \"26-35\", \"36-45\", \"46+\"]\n",
    "df['age_group'] = pd.cut(df['customer_age'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "# Calculate mean purchase amount for each age group (fixing the warning)\n",
    "mean_purchase_per_group = df.groupby('age_group', observed=False)['purchase_amount'].mean()\n",
    "print(\"Mean Purchase Amount per Age Group:\")\n",
    "print(mean_purchase_per_group)\n",
    "\n",
    "# ANOVA test\n",
    "grouped_data = [df[df['age_group'] == age]['purchase_amount'] for age in labels]\n",
    "anova_stat, anova_p = f_oneway(*grouped_data)\n",
    "\n",
    "print(\"\\nANOVA Test Results:\")\n",
    "print(f\"Test Statistic: {anova_stat:.4f}, P-Value: {anova_p:.4f}\")\n",
    "\n",
    "if anova_p < alpha:\n",
    "    print(\"Reject H0: Significant differences exist in purchase amounts across age groups.\")\n",
    "else:\n",
    "    print(\"Fail to reject H0: No significant differences in purchase amounts across age groups.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57881ec4-036d-44c4-a5db-fb3c17fe0285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
